{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa86f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph as LangChainNeo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f10a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "NEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\")\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.environ.get(\"NEO4J_USERNAME\")\n",
    "NEO4J_DATABASE = os.environ.get(\"NEO4J_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e94e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4e290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_rag_chain():\n",
    "    if not GROQ_API_KEY or not NEO4J_PASSWORD:\n",
    "        raise ValueError(\"API key or password not found\")\n",
    "    \n",
    "    graph = LangChainNeo4jGraph(\n",
    "        url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    "    )\n",
    "    graph.refresh_schema()\n",
    "\n",
    "    llm = ChatGroq(temperature=0, model= \"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
    "\n",
    "    embeddings = hf_embeddings\n",
    "    vector_store = Neo4jVector(\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD,\n",
    "        database=NEO4J_DATABASE,\n",
    "        embedding=embeddings,\n",
    "        index_name=\"monograph_chunks\", # This must match the index name from the ingestion and vectorization notebbok\n",
    "        node_label=\"Chunk\",\n",
    "        text_node_property=\"text\",\n",
    "        embedding_node_property=\"embedding\",\n",
    "    )\n",
    "    \n",
    "    # Graph cypher QA chain for structured queries\n",
    "    graph_chain = GraphCypherQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        #qa_llm=llm,\n",
    "        graph=graph,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        allow_dangerous_requests=True\n",
    "    )\n",
    "\n",
    "    vector_retriever = vector_store.as_retriever()\n",
    "\n",
    "    return graph_chain, vector_retriever, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19399ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def query_hybrid_rag(question, history):\n",
    "    try:\n",
    "        # Vector search to find relevant text chunks\n",
    "        retrieved_docs= vector_retreiver.invoke(question)\n",
    "        retrieved_context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Graph search to find structured data\n",
    "        graph_result = graph_chain.invoke({\"query\": question})\n",
    "        graph_answer = graph_result[\"result\"]\n",
    "        cypher_query = graph_result[\"intermediate_steps\"][0].get(\"query\", \"could not generate cypher query\")\n",
    "\n",
    "        # Synthesize the final answer using both contexts\n",
    "        synthesis_prompt_template = \"\"\"\n",
    "        You are an expert Q&A assistant. Use the following context from a document and a knowledge graph to answer the question.\n",
    "        Provide a comprehensive answer that combines insights from both sources. If the contexts are empty or irrelevant, say you don't have enough information.\n",
    "\n",
    "        Vector Search Context (Unstructured text from the document):\n",
    "        {vector_context}\n",
    "\n",
    "        Knowledge Graph Context (Structured data from the graph):\n",
    "        {graph_context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Comprehensive Answer:\n",
    "        \"\"\"\n",
    "\n",
    "        synthesis_prompt = PromptTemplate(\n",
    "            template=synthesis_prompt_template,\n",
    "            input_variables=[\"vector_context\", \"graph_context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        synthesis_chain = synthesis_prompt | llm\n",
    "\n",
    "        final_answer_obj = synthesis_chain.invoke({\n",
    "            \"vector_context\": retrieved_context,\n",
    "            \"graph_context\": graph_answer,\n",
    "            \"question\": question\n",
    "        })\n",
    "\n",
    "        final_answer = final_answer_obj.content      \n",
    "\n",
    "        formatted_output = (\n",
    "            f\"**Final Answer:**\\n{final_answer}\\n\\n\"\n",
    "            f\"---\\n\"\n",
    "            f\"**Retrieved from Knowledge Graph:**\\n{graph_answer}\\n\\n\"\n",
    "            f\"**Generated Cypher Query:**\\n```cypher\\n{cypher_query}\\n```\\n\\n\"\n",
    "            f\"**Retrieved from Vector Search (Top Chunks):**\\n> {retrieved_context.replace(chr(10), chr(10) + '> ')}\"    \n",
    "        )\n",
    "\n",
    "        return formatted_output\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}]\")\n",
    "        return f\"an error occurred: {e}. please ensure neo4j database is running\"\n",
    "    \n",
    "\n",
    "# GRADIO INTERFACE\n",
    "if __name__ == \"__main__\":\n",
    "        print(\"Initializing Hybrid GraphRAG chain...\")\n",
    "        try:\n",
    "            graph_chain, vector_retreiver, llm=create_hybrid_rag_chain()\n",
    "            print(\"Chain initialized successfully\")\n",
    "\n",
    "            with gr.Blocks(theme=gr.themes.Soft(), title=\"Hybrid Cancer Monograph\") as demo:\n",
    "                gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Hybrid GraphRAG Q&A for Cancer Monograph\n",
    "                This system uses both vector search and a knowledge graph to provide comprehensive answers.\n",
    "                \"\"\"\n",
    "                )\n",
    "                gr.ChatInterface(\n",
    "                fn=query_hybrid_rag,\n",
    "                title=\"Ask the Monograph (Hybrid)\",\n",
    "                chatbot=gr.Chatbot(height=500),\n",
    "                textbox=gr.Textbox(placeholder=\"e.g., What is the link between betel quid chewing and breast cancer in the North-East?\", container=False, scale=7),\n",
    "                examples=[\n",
    "                    \"What are the leading sites of cancer in Mizoram?\",\n",
    "                    \"Which studies did the ICMR conduct on tobacco?\",\n",
    "                    \"What is the link between the GSTM1 gene and oral cancer?\"\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            print(\"\\n Launching Gradio App.. open the URL in your browser\")\n",
    "            demo.launch(share=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"falied to initialize the application: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
