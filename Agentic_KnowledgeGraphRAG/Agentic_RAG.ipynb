{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f0bc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph \n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "from langchain.chains.router import RouterChain\n",
    "from langchain.chains.router.multi_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.llm import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba6a72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "NEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\")\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.environ.get(\"NEO4J_USERNAME\")\n",
    "NEO4J_DATABASE = os.environ.get(\"NEO4J_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01a5acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94a5057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Agentic RAG chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\Graph_RAG_&_Hybrid_GraphRAG\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic chain initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_2560\\1904678769.py:140: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=500),\n",
      "d:\\Projects\\Graph_RAG_&_Hybrid_GraphRAG\\env\\Lib\\site-packages\\gradio\\chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Launching Gradio App... Open the URL in your browser.\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_agentic_rag_components():\n",
    "    if not GROQ_API_KEY or not NEO4J_PASSWORD:\n",
    "        raise ValueError(\"API keys or password not found\")\n",
    "    \n",
    "    llm = ChatGroq(temperature=0, model = \"meta-llama/llama-4-maverick-17b-128e-instruct\")\n",
    "\n",
    "    # --- TOOL 1 ---\n",
    "    embeddings = hf_embeddings\n",
    "    vector_store = Neo4jVector.from_existing_index(\n",
    "        url=NEO4J_URI,\n",
    "        username=NEO4J_USERNAME,\n",
    "        password=NEO4J_PASSWORD,\n",
    "        database=NEO4J_DATABASE,\n",
    "        embedding=embeddings,\n",
    "        index_name=\"monograph_chunks\"\n",
    "    )\n",
    "\n",
    "    vector_retriever = vector_store.as_retriever()\n",
    "\n",
    "    vector_qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm, chain_type=\"stuff\", retriever=vector_retriever, return_source_documents=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    #---TOOL 2---\n",
    "    graph = Neo4jGraph(\n",
    "        url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    "    )\n",
    "\n",
    "    graph.refresh_schema()\n",
    "    \n",
    "    graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=graph,\n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        allow_dangerous_requests=True\n",
    "    )\n",
    "\n",
    "    #Descriptions for the Router\n",
    "    prompt_infos = [\n",
    "        {\n",
    "            \"name\": \"vector_search\",\n",
    "            \"description\": \"Good for answering broad, semantic questions or general topics about cancer research.\",\n",
    "            \"prompt\": PromptTemplate.from_template(\"{input}\")\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"graph_search\",\n",
    "            \"description\": \"Good for answering specific questions about entities and their relationships, like 'What are the leading cancers in a specific city?'.\",\n",
    "            \"prompt\": PromptTemplate.from_template(\"{input}\")\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # --- Create the Router Chain ---\n",
    "    destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "    destinations_str = \"\\n\".join(destinations)\n",
    "    router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "    router_prompt = PromptTemplate(\n",
    "        template=router_template,\n",
    "        input_variables=[\"input\"],\n",
    "        output_parser=RouterOutputParser(),\n",
    "    )\n",
    "    router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "    # Create a dictionary to map tool names to their actual chains\n",
    "    destination_chains = {\n",
    "        \"vector_search\": vector_qa_chain,\n",
    "        \"graph_search\": graph_qa_chain,\n",
    "    }\n",
    "\n",
    "    return router_chain, destination_chains\n",
    "\n",
    "\n",
    "def query_agentic_rag(question, history):\n",
    "    if not question or not question.strip():\n",
    "        return \"Please enter a question\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Use the router to decide which tool to use\n",
    "        router_result = router_chain.invoke({\"input\": question})\n",
    "        tool_name = router_result['destination']\n",
    "        \n",
    "        if tool_name not in destination_chains:\n",
    "             # Fallback to a default if the router is confused\n",
    "            tool_name = \"vector_search\"\n",
    "\n",
    "        # 2. Get the appropriate chain and run it\n",
    "        destination_chain = destination_chains[tool_name]\n",
    "        # Use 'query' for QA chains and 'input' for router\n",
    "        result = destination_chain.invoke({\"query\": question})\n",
    "\n",
    "        answer = result.get(\"result\", \"I could not find a definitive answer.\")\n",
    "        details = \"\"\n",
    "\n",
    "        if tool_name == \"Graph Search\":\n",
    "            intermediate_steps = result.get(\"intermediate_steps\", [])\n",
    "            cypher_query = intermediate_steps[0].get(\"query\", \"N/A\") if intermediate_steps else \"N/A\"\n",
    "            details = f\"**Generated Cypher Query:**\\n```cypher\\n{cypher_query}\\n```\"\n",
    "\n",
    "        elif tool_name == \"Vector Search\":\n",
    "            source_docs = result.get(\"source_documents\", [])\n",
    "            context_preview = \"\\n\\n\".join([doc.page_content[:300] + \"...\" for doc in source_docs[:3]])\n",
    "            details = (\n",
    "                f\"**Retrieved from Vector Search (Top Chunks):**\\n\"\n",
    "                f\"> {context_preview.replace(chr(10), chr(10) + '> ')}\"\n",
    "            )   \n",
    "            \n",
    "\n",
    "        return (\n",
    "            f\"**Answer:**\\n{answer}\\n\\n\"\n",
    "            f\"---\\n\"\n",
    "            f\"**Tool Used:** {tool_name.replace('_', ' ').title()}\\n\\n\"\n",
    "            f\"{details}\"\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "        return (\n",
    "            f\"An error occurred: {e}. \"\n",
    "            f\"Please ensure your Neo4j database is running, credentials are correct, and the API is reachable.\"\n",
    "        )\n",
    "    \n",
    "# Gradio Interface\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing Agentic RAG chain...\")\n",
    "    try:\n",
    "        router_chain, destination_chains = create_agentic_rag_components()\n",
    "        print(\"Agentic chain initialized successfully.\")\n",
    "\n",
    "        with gr.Blocks(theme=gr.themes.Soft(), title=\"Agentic Cancer Monograph RAG\") as demo:\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                # ðŸ§  Agentic GraphRAG Q&A\n",
    "                This system uses an AI agent to decide whether to use vector search or a knowledge graph query to best answer your question.\n",
    "                \"\"\"\n",
    "            )\n",
    "            gr.ChatInterface(\n",
    "                fn=query_agentic_rag,\n",
    "                title=\"Ask the Monograph (Agentic)\",\n",
    "                chatbot=gr.Chatbot(height=500),\n",
    "                textbox=gr.Textbox(placeholder=\"e.g., What is the link between betel quid chewing and breast cancer?\", container=False, scale=7),\n",
    "                examples=[\n",
    "                    \"What are the leading sites of cancer in Mizoram?\",\n",
    "                    \"Tell me about the role of diet in cancer.\",\n",
    "                    \"Which studies did the ICMR conduct on tobacco?\"\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        print(\"\\nðŸš€ Launching Gradio App... Open the URL in your browser.\")\n",
    "        demo.launch(share=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize the application: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf6be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
